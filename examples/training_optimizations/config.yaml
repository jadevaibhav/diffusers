# Training configuration
training:
  output_dir: "ddpm-model-64"
  overwrite_output_dir: false
  cache_dir: null
  num_epochs: 100
  save_images_epochs: 10
  save_model_epochs: 10
  gradient_accumulation_steps: 1
  train_batch_size: 16
  eval_batch_size: 16
  dataloader_num_workers: 0
  resume_from_checkpoint: null
  checkpointing_steps: 500
  checkpoints_total_limit: null
  local_rank: -1
  logger: "tensorboard"
  logging_dir: "logs"
  mixed_precision: "no"  # Options: no, fp16, bf16
  push_to_hub: false
  hub_model_id: null
  hub_private_repo: false
  hub_token: null
  enable_xformers_memory_efficient_attention: false
  use_ema: false

# Image augmentation configuration
augmentation:
  resolution: 64
  center_crop: false
  random_flip: false

# Learning rate and scheduler configuration
lr_scheduler:
  learning_rate: 1e-4
  lr_scheduler_type: "cosine"  # Options: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup
  lr_warmup_steps: 500
  adam_beta1: 0.95
  adam_beta2: 0.999
  adam_weight_decay: 1e-6
  adam_epsilon: 1e-08

# DDPM noise schedule configuration
ddpm:
  num_steps: 1000
  num_inference_steps: 1000
  beta_schedule: "linear"  # Options: linear, cosine
  prediction_type: "epsilon"  # Options: epsilon, sample, v_prediction
  rescale_betas_zero_snr: false
# EMA (Exponential Moving Average) configuration
ema:
  inv_gamma: 1.0
  power: 0.75
  max_decay: 0.9999

# Dataset configuration
dataset:
  dataset_name: null
  dataset_config_name: null
  train_data_dir: null